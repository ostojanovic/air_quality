{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c087eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches \n",
    "import calendar\n",
    "\n",
    "import datetime\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e61d00",
   "metadata": {},
   "source": [
    "# Modeling concentration of PM2.5 across counties in California"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f4e86",
   "metadata": {},
   "source": [
    "__[Particulate matter (PM)](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics)__ refers to tiny particles of solid or liquid suspended in the air. Particles with a diameter of 2.5 micrometers or smaller (PM2.5) are __[particularly concerning](https://ww2.arb.ca.gov/resources/inhalable-particulate-matter-and-health)__ because they can reach deep into the lungs and even the bloodstream. Exposure to PM2.5 has been linked to stroke, cardiovascular and respiratory disease, cancer, and other serious health outcomes. \n",
    "\n",
    "While daily concentrations above 35 μg/m³ are considered extremely hazardous, there is no known safe level of PM2.5 exposure.\n",
    "\n",
    "PM2.5 comes from a wide range of sources, either emitted directly from vehicles, industrial activity, or fires, or formed in the atmosphere through chemical reactions involving pollutants like sulfur dioxide or nitrogen oxides. In California, wildfires are a major source. Ash and smoke contain __[high levels of PM2.5](https://www.epa.gov/wildfire-smoke-course/why-wildfire-smoke-health-concern)__, and wildfire events have become more frequent and intense with climate change.\n",
    "\n",
    "In this notebook, we’ll build Bayesian hierarchical models to predict average monthly PM2.5 concentrations at the county level in California. We will use publicly available data provided by the U.S. __[Environmental Protection Agency (EPA)](https://www.epa.gov/outdoor-air-quality-data/download-daily-data)__. Our goal is to account for spatial and temporal variation and eventually quantify uncertainty in these predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684606cb",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5109e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ' '   # change this to your folder path\n",
    "dataframes = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "data = pd.concat(dataframes, ignore_index=True).sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82ac38",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c146d",
   "metadata": {},
   "source": [
    "First, we’ll do a few quick checks: are there missing values, any NaNs, anything that looks unsual. If some data is missing, we’ll have to decide whether to drop or impute. We’ll also look at the distributions of the main variables because we need that to pick a reasonable likelihood for the Bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb402d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036d6db",
   "metadata": {},
   "source": [
    "It seems like we have some negative value in `Daily Mean PM2.5 Concentration` column, which shouldn't happen. Let's inspect those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[data[\"Daily Mean PM2.5 Concentration\"]<0][\"Daily Mean PM2.5 Concentration\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea23add",
   "metadata": {},
   "source": [
    "Negative values usually come from equipment malfunction, and we will in this tutorial replace them with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d9de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"Daily Mean PM2.5 Concentration\"] < 0, \"Daily Mean PM2.5 Concentration\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae85c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"Daily Mean PM2.5 Concentration\"]<0][\"Daily Mean PM2.5 Concentration\"].sum() # the sum should we zero, we successfully replaced all negative values with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a96fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any() # there are some NaNs in the dataset, but we're lucky, we don't have any NaNs columns we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14396438",
   "metadata": {},
   "source": [
    "We want to fit a model on data from 2019 to 2024, and treat everything we have in 2025 as 'unseen data'. First, we need to ensure we have data for all counties during that period, as monitoring stations can sometimes go offline, be decommissioned, or fail to collect data for certain stretches of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab80853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data for 2025\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "rows_to_drop = data[data['Date'] > datetime.datetime(2024, 12, 31)].index\n",
    "\n",
    "data = data.drop(rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45479458",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_list = data['County'].unique().tolist()\n",
    "\n",
    "print(\"These counties have some data missing in the last 6 years:\")\n",
    "\n",
    "missing_counties = []\n",
    "missing_dates_expanded = []\n",
    "percentage_missing_list = [] \n",
    "\n",
    "# Define the expected date range for the last 6 years\n",
    "start_date = \"2019-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "expected_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "for county in counties_list:\n",
    "    county_data = data[data['County'] == county]\n",
    "    \n",
    "    # Ensure only unique dates are considered\n",
    "    actual_dates = pd.to_datetime(county_data['Date']).drop_duplicates().sort_values()\n",
    "    \n",
    "    # Find missing dates by comparing expected and actual dates\n",
    "    missing_dates = expected_dates.difference(actual_dates)\n",
    "    \n",
    "    if len(missing_dates) > 0: \n",
    "        missing_counties.append(county)\n",
    "        percentage_missing = np.round((100 - len(actual_dates) / len(expected_dates) * 100), 2)\n",
    "        percentage_missing_list.append({\"County\": county, \"Percentage Missing\": percentage_missing})\n",
    "        \n",
    "        for date in missing_dates:\n",
    "            missing_dates_expanded.append({\"County\": county, \"Missing Date\": date})\n",
    "        print(county, \"has\", percentage_missing, \"% of data missing\")\n",
    "\n",
    "missing_dates_df = pd.DataFrame(missing_dates_expanded)\n",
    "percentage_missing_df = pd.DataFrame(percentage_missing_list)\n",
    "missing_dates_df = missing_dates_df.merge(percentage_missing_df, on=\"County\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78cc5bb",
   "metadata": {},
   "source": [
    "Most counties with missing dates have a very low percentage of missing data, which probably means there’s just an occasional missing date. In those cases, we can impute average values.\n",
    "\n",
    "However, a few counties have a substantially higher proportion of missing dates, like Napa (61.82%) or Humboldt (53.6%). This likely indicates that monitoring stations went offline for an extended period. In such cases, it doesn’t make sense to impute average values. A common rule of thumb is that imputation is reasonable when less than 25% of the data is missing, although  this heavily depends on the context.\n",
    "\n",
    "In our case, Shasta County has 25.55% missing data. That’s borderline, so we could reasonably go either way. I decided to keep it, since we're already excluding a few other counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_to_drop = percentage_missing_df[percentage_missing_df[\"Percentage Missing\"] >= 26][\"County\"].tolist()\n",
    "\n",
    "data = data[~data[\"County\"].isin(counties_to_drop)].reset_index(drop=True)\n",
    "\n",
    "remaining_counties = data[\"County\"].unique()\n",
    "print(f\"Remaining counties: {len(remaining_counties)}\")\n",
    "print(f\"Counties still in data: {remaining_counties}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c870f",
   "metadata": {},
   "source": [
    "### Impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f458b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out counties to drop\n",
    "missing_dates_df = missing_dates_df[~missing_dates_df['County'].isin(counties_to_drop)].reset_index(drop=True)\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "for county in missing_dates_df['County'].unique():\n",
    "    county_missing_dates = missing_dates_df[missing_dates_df['County'] == county]['Missing Date']\n",
    "    \n",
    "    # Get a sample row for the county to copy other column values\n",
    "    sample_row = data[data['County'] == county].iloc[0].to_dict()\n",
    "    \n",
    "    # Calculate mean values for specific columns\n",
    "    mean_values = data[data['County'] == county][['Daily Mean PM2.5 Concentration', 'Daily AQI Value', 'Daily Obs Count']].mean()\n",
    "\n",
    "    # Create new rows for missing dates\n",
    "    for missing_date in county_missing_dates:\n",
    "        new_row = sample_row.copy() \n",
    "        new_row['Date'] = missing_date \n",
    "        new_row.update(mean_values.to_dict())\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "data = pd.concat([data, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "data = data.sort_values(by=['County', 'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d92bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing dates in the entire data DataFrame\n",
    "start_date = \"2019-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "expected_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "actual_dates = pd.to_datetime(data['Date']).drop_duplicates().sort_values()\n",
    "missing_dates = expected_dates.difference(actual_dates)\n",
    "\n",
    "if len(missing_dates) > 0:\n",
    "    print(f\"The dataset still has {len(missing_dates)} missing dates.\")\n",
    "else:\n",
    "    print(\"The dataset has no missing dates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b54d78",
   "metadata": {},
   "source": [
    "### Time-series of daily PM2.5 concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=data, x=\"Date\", y=\"Daily Mean PM2.5 Concentration\")\n",
    "\n",
    "tick_step = 90 # put ticks every 3 months\n",
    "unique_dates = pd.Series(data['Date'].unique()) \n",
    "plt.xticks(ticks=unique_dates[::tick_step], \n",
    "           labels=unique_dates[::tick_step].dt.strftime('%Y-%m'), \n",
    "           rotation=45)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"PM2.5 Concentration\")\n",
    "plt.title(\"Daily Average PM2.5 Concentration in California (2019-2024)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902c3da",
   "metadata": {},
   "source": [
    "We see that on most days, daily concentration in California is around 20 μg/m³ or less. There are some big spikes in 2020 and 2021, which probably come from a big wildfire or similar extreme event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c874209",
   "metadata": {},
   "source": [
    "### Does data have seasonality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14ca07",
   "metadata": {},
   "source": [
    "Before developing a model, we should check whether the data exhibits any seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"month_name\"] = data[\"Date\"].dt.strftime('%b')\n",
    "data[\"month_num\"] = data[\"Date\"].dt.month  # for correct ordering\n",
    "\n",
    "# Group by month number to preserve order\n",
    "monthly_seasonal_avg = data.groupby(\"month_num\")[\"Daily Mean PM2.5 Concentration\"].mean().reset_index()\n",
    "\n",
    "monthly_seasonal_avg[\"month_name\"] = monthly_seasonal_avg[\"month_num\"].apply(lambda x: calendar.month_abbr[x]) \n",
    "\n",
    "sns.lineplot(x='month_name', y='Daily Mean PM2.5 Concentration', data=monthly_seasonal_avg, marker='o', color='teal')\n",
    "plt.title(\"Average PM2.5 by Calendar Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average PM2.5\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5619f",
   "metadata": {},
   "source": [
    "We see higher average monthly concentration of PM2.5 in September and toward the end of the year and in January. \n",
    "\n",
    "This pattern likely reflects a combination of factors: \n",
    "\n",
    "* wildfire activity in late summer and fall, driven in part by strong dry winds such as the __[Santa Ana winds](https://en.wikipedia.org/wiki/Santa_Ana_winds)__ in Southern California and __[Diablo winds](https://en.wikipedia.org/wiki/Diablo_wind)__ in Northern California, which rapidly spread fires and worsen air quality\n",
    "\n",
    "* winter pollution events like __[temperature inversions](https://www.eea.europa.eu/en/analysis/maps-and-charts/temperature-inversion-traps-pollution-at)__ and residential heating contribute to elevated PM2.5 levels in the colder months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2ce23",
   "metadata": {},
   "source": [
    "## Calculate monthly average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ad6e0",
   "metadata": {},
   "source": [
    "We have daily data, but for the rest of the tutorial we’ll use monthly averages. Using monthly data reduces computational load and helps the model run faster. It also smooths out short-term fluctuations, which makes it easier to focus on longer-term trends.\n",
    "\n",
    "We will now calculate monthly average concentration of PM2.5 and some variables that will help us incorproate seasonality into our model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6216e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'county' and 'year_month'and extract the first date for each county\n",
    "# We know that first date everywhere should be 2019-01-01, \n",
    "# but let's do it automatically just in case one county has no data for that month \n",
    "data = data.sort_values(by=['County', 'Date'])\n",
    "\n",
    "first_dates = data.groupby('County')['Date'].first().reset_index()\n",
    "first_dates.rename(columns={'Date': 'first_date'}, inplace=True)  # Rename the column to 'first_date'\n",
    "\n",
    "data = data.merge(first_dates, on='County', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "190ab6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total time since beginning of time-serials (in years)\n",
    "# This will be later used to calculate seasonality and trend\n",
    "\n",
    "data[\"time_passed_total\"] = (\n",
    "    data['Date'].apply(datetime.datetime.toordinal)\n",
    "    -data['first_date'].apply(datetime.datetime.toordinal)\n",
    ")/365.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe161368",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()\n",
    "\n",
    "data_copy['year_month'] = data_copy['Date'].dt.strftime('%Y-%m')\n",
    "\n",
    "monthly_avg = data_copy.groupby(['County', 'year_month'])['Daily Mean PM2.5 Concentration'].mean().reset_index()\n",
    "\n",
    "monthly_avg.rename(columns={'Daily Mean PM2.5 Concentration': 'Monthly Avg PM2.5'}, inplace=True)\n",
    "\n",
    "first_dates = monthly_avg.groupby('County')['year_month'].first().reset_index()\n",
    "first_dates.rename(columns={'year_month': 'first_date'}, inplace=True)\n",
    "\n",
    "monthly_avg = monthly_avg.merge(first_dates, on='County', how='left')\n",
    "\n",
    "monthly_avg['year_month'] = pd.to_datetime(monthly_avg['year_month'], format='%Y-%m')\n",
    "monthly_avg['first_date'] = pd.to_datetime(monthly_avg['first_date'], format='%Y-%m')\n",
    "\n",
    "# Calculate time passed in years since county's first month (for trend)\n",
    "monthly_avg[\"time_passed_total\"] = (\n",
    "    monthly_avg['year_month'].apply(datetime.datetime.toordinal)\n",
    "    - monthly_avg['first_date'].apply(datetime.datetime.toordinal)\n",
    ") / 365.0\n",
    "\n",
    "\n",
    "monthly_avg['month'] = monthly_avg['year_month'].dt.month\n",
    "\n",
    "# Normalize month to [0,1) for seasonality\n",
    "monthly_avg['month_fraction'] = (monthly_avg['month'] - 1) / 12.0\n",
    "\n",
    "monthly_avg['county_id'] = monthly_avg['County'].astype('category').cat.codes\n",
    "n_counties = monthly_avg['county_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how our new dataframe looks like\n",
    "monthly_avg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4968f",
   "metadata": {},
   "source": [
    "### Plot distribution of monthly average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(monthly_avg[\"Monthly Avg PM2.5\"], kde=True)\n",
    "\n",
    "plt.title(\"Distribution of Monthly Average PM2.5 Concentration in California (2019-2024)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd70bb3",
   "metadata": {},
   "source": [
    "As expected, many data points are between 0 and 20. There's also a small bump between 20 and 50, and a long tail. Let's plot a distribution for a couple of counties to get a better sense of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = ['Los Angeles', 'Fresno', 'San Francisco', 'San Diego']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, county in zip(axes.flatten(), counties):\n",
    "    county_data = monthly_avg[monthly_avg['County'] == county]\n",
    "    sns.kdeplot(np.log(county_data['Monthly Avg PM2.5']), color='cornflowerblue', ax=ax)\n",
    "    ax.set_title(county)\n",
    "    ax.set_xlabel('Monthly Avg PM2.5')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932b081",
   "metadata": {},
   "source": [
    "Looking at these distributions, it’s clear the data doesn’t follow a Normal distribution. It looks like it might be bimodal. It's probably a mix of two distributions, which would make sense. On most days PM2.5 is low, but during wildfires it can get very high.\n",
    "\n",
    "It seems like on \"good days\" data follows a __[LogNormal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)__ (no negative values), but bad days it probably follows a different LogNormal with a higher mean. That pattern suggests there might be some kind of threshold or shift when extreme events occur.\n",
    "\n",
    "We will now use this information to fit a Bayesian model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d76a08",
   "metadata": {},
   "source": [
    "## Fitting a Bayesian hierarchical model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99c011",
   "metadata": {},
   "source": [
    "### Bayesian hierarchical models\n",
    "\n",
    "__[Bayesian hierarchical models](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling)__ are a class of statistical models that involve multiple levels of parameters, allowing for the modeling of complex, structured data. They are particularly useful when data is organized in groups or layers, such as nested or repeated measurements, or when modeling different regions that share common characteristics. In these models, parameters at one level of the hierarchy depend on parameters at a higher level, facilitating the sharing of information across different groups. This approach leads to more accurate and robust inferences, especially when some groups, have limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604bc5d",
   "metadata": {},
   "source": [
    "### Fitting a model in PyMC\n",
    "\n",
    "We will build our models using __[PyMC](https://www.pymc.io/projects/docs/en/stable/learn.html)__, a python package for Bayesian inference. \n",
    "\n",
    "We’ll use PyMC, a Python library for building Bayesian models and running inference with MCMC. It’s useful when you want to define probabilistic models explicitly, use priors, and get full posterior distributions instead of point estimates. PyMC lets you write models in code that closely mirrors their mathematical structure, and it supports advanced features like hierarchical models, custom likelihoods, and missing data handling.\n",
    "\n",
    "If you're starting with PyMC, I suggest reading about __[its core features](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/index.html)__ , the __[Introductiory Overview of PyMC](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html)__ or how to fit a __[GLM regression](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html)__ in PyMC.\n",
    "\n",
    "\n",
    "In PyMC, models are defined inside a `pm.Model()` context manager. All components (priors, likelihoods, transformations, and observed data) must be declared within this block so PyMC can track them and handle inference properly.\n",
    "\n",
    "Within the model context, we will define:\n",
    "\n",
    "* **Priors** - distributions that reflect our assumptions before seeing the data,\n",
    "\n",
    "* **The expected value and standard deviation** of the likelihood, and\n",
    "\n",
    "* **The likelihood function** itself, which links parameters to the observed data.\n",
    "\n",
    "To fit the model, we will use __[Markov Chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)__. Directly computing the posterior distribution is infeasible for most real-world models, so we approximate it using MCMC. This generates a sequence of samples (called *draws*) from the posterior distribution, which we can then use to estimate parameter values, uncertainty intervals, or make predictions.\n",
    "\n",
    "Each **draw** is one sample from the posterior, and represents **one possible combination of values for all the model’s parameters at once**. This means that for every parameter in the model (e.g., intercepts, slopes, variances, or group-level effects), you get one specific value in that draw. \n",
    "\n",
    "So, when PyMC reports that \"*50 draws are finished*\" it means the sampler has generated 50 such samples and each contains values for all parameters. These 50 draws together represent 50 different possible sets of parameter values drawn from the posterior distribution.\n",
    "\n",
    "A **chain** is an independent sequence of such draws. Running multiple chains (e.g. `chains=4`) helps us check whether sampling has converged (i.e., whether different chains explore the same distribution).\n",
    "\n",
    "The output of `pm.sample()` is called a **trace**. It contains all the MCMC draws across all chains and parameters.\n",
    "\n",
    "You can think of the trace as a structured container (like a multidimensional array or cube), which stores all sampled combinations of model parameters. In hierarchical models, it represents an entire posterior predictive distribution for each group or time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f4cbc",
   "metadata": {},
   "source": [
    "## Model 1: No hierarchy\n",
    "\n",
    "To start, we’ll fit a non-hierarchical model. This simpler version will make it easier to explore PyMC’s features and be our a sanity check when we compare it to more complex models.\n",
    "\n",
    "In this model, we include seasonality because the data clearly shows a strong seasonal pattern. We also define an intercept to capture the baseline level.\n",
    "\n",
    "Since most days in our data don’t contain extreme values but occasional extreme events do occur, we model the probability of extremes using a __[Bernoulli distribution](https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.Bernoulli.html)__. Depending on whether an extreme event occurs, the model switches between two __[LogNormal](https://www.pymc.io/projects/docs/en/stable/api/distributions/generated/pymc.LogNormal.html)__ likelihoods, one for typical days and one for extreme days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_no_hierarchy:\n",
    "\n",
    "    # --- Priors for seasonality ---\n",
    "    # We model seasonal effects using sine and cosine terms to capture cyclical patterns over months.\n",
    "    # The month_fraction variable represents the time within a year as a fraction (e.g., 0.0 to 1.0).\n",
    "    # Taking sine and cosine of 2π * month_fraction captures annual seasonality.\n",
    "    \n",
    "    sine = pm.math.sin(2 * np.pi * (monthly_avg[\"month_fraction\"].values % 1))\n",
    "    cosine = pm.math.cos(2 * np.pi * (monthly_avg[\"month_fraction\"].values % 1))\n",
    "\n",
    "    # We assign Normal priors to the coefficients of the sine and cosine terms,\n",
    "    # reflecting uncertainty about the amplitude of seasonal effects.\n",
    "    w_s = pm.Normal(\"w_s\", mu=0, sigma=1)\n",
    "    w_c = pm.Normal(\"w_c\", mu=0, sigma=1)\n",
    "\n",
    "    # The overall seasonal effect is a linear combination of sine and cosine terms weighted by w_s and w_c.\n",
    "    seasonality = sine * w_s + cosine * w_c\n",
    "\n",
    "    # --- Priors for intercept ---\n",
    "    # The intercept represents the baseline log-level of PM2.5 without seasonality or extremes.\n",
    "    # We define a with a wide prior for the mean intercept (intercept_mu)\n",
    "    # and a narrower offset term (intercept_offset) to allow some flexibility.\n",
    "\n",
    "    intercept_mu = pm.Normal(\"intercept_mu\", mu=0, sigma=5)  # wide prior on mean\n",
    "    intercept_offset = pm.Normal(\"intercept_offset\", mu=0, sigma=1)\n",
    "\n",
    "    intercept = intercept_mu + intercept_offset\n",
    "\n",
    "    # --- Modeling extreme events ---\n",
    "    # We create a binary indicator variable 'is_extreme' to capture days with unusually high PM2.5.\n",
    "    # The probability of an extreme event is modeled with a Beta prior to allow it to be low but flexible.\n",
    "    \n",
    "    p_is_extreme = pm.Beta(\"p_is_extreme\", alpha=1, beta=20)\n",
    "    is_extreme = pm.Bernoulli(\"is_extreme\", p_is_extreme, shape=monthly_avg.shape[0])  \n",
    "\n",
    "    # --- Expected value of PM2.5 ---\n",
    "    # The baseline expected log PM2.5 is the sum of intercept and seasonal effect (mu_base).\n",
    "   \n",
    "    mu_base = intercept + seasonality\n",
    "\n",
    "    # For extreme events, we allow a separate, higher mean value (mu_extreme) with a fairly wide prior.\n",
    "    mu_extreme = pm.Normal(\"mu_extreme\", mu=5, sigma=5)\n",
    "\n",
    "    # We switch between baseline and extreme expected values depending on 'is_extreme'.\n",
    "    mu = pm.math.switch(is_extreme, mu_extreme, mu_base)\n",
    "\n",
    "    # --- Priors for standard deviations ---\n",
    "    # We model measurement noise or natural variability with two HalfNormal priors:\n",
    "    # one for the baseline condition (sigma_base), and one for extreme events (sigma_extreme).\n",
    "\n",
    "    sigma_base = pm.HalfNormal(\"sigma_base\", sigma=1)\n",
    "    sigma_extreme = pm.HalfNormal(\"sigma_extreme\", sigma=1)\n",
    "\n",
    "    # We switch the standard deviation based on 'is_extreme' status.\n",
    "\n",
    "    sigma = pm.math.switch(is_extreme, sigma_extreme, sigma_base)\n",
    "\n",
    "    # --- Likelihood ---\n",
    "    # The observed PM2.5 values are modeled as LogNormal distributed,\n",
    "    # since PM2.5 concentrations are positive and often right-skewed.\n",
    "    # The LogNormal distribution uses the 'mu' and 'sigma' defined above.\n",
    "  \n",
    "    Y = pm.LogNormal(\"Y\", \n",
    "                    mu=mu, \n",
    "                    sigma=sigma, \n",
    "                    observed=monthly_avg['Monthly Avg PM2.5'])\n",
    "    \n",
    "    # --- Sampling ---\n",
    "    # We draw 1000 posterior samples per chain after 500 tuning steps,\n",
    "    # using 4 independent chains run in parallel on 4 CPU cores.\n",
    "\n",
    "    trace_no_hierarchy = pm.sample(1000, tune=500, chains=4, cores=4, return_inferencedata=True)\n",
    "\n",
    "    # --- Compute log likelihood ---\n",
    "    # This is useful for model comparison or further diagnostics.\n",
    "    pm.compute_log_likelihood(trace_no_hierarchy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b852f6",
   "metadata": {},
   "source": [
    "### Inspect trace object and visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_no_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the posterior samples directly\n",
    "trace_no_hierarchy.posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the samples for 'intercept_mu'\n",
    "trace_no_hierarchy.posterior[\"intercept_mu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dimensions: \", trace_no_hierarchy.posterior[\"intercept_mu\"].dims)\n",
    "print(\"shape: \", trace_no_hierarchy.posterior[\"intercept_mu\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also print the model object to inspect its structure.\n",
    "# This shows the model's components (priors, likelihoods, etc.) and how they are connected.\n",
    "# It’s useful for verifying that everything is defined as expected.\n",
    "\n",
    "model_no_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualize the model structure using Graphviz.\n",
    "# This generates a computational graph showing how all variables are connected in the model\n",
    "\n",
    "pm.model_to_graphviz(model_no_hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab00e63",
   "metadata": {},
   "source": [
    "### Convergence checks\n",
    "\n",
    "The R-hat statistic, also known as the __[Gelman-Rubin diagnostic](https://en.wikipedia.org/wiki/Gelman-Rubin_statistic)__, is used to assess the convergence of MCMC chains. It compares the variance within each chain to the variance between chains. If the chains have converged, the within-chain variance should be similar to the between-chain variance, and R-hat should be close to 1. An R-hat value significantly greater than 1.01 indicates that the chains have not yet converged, suggesting that more iterations may be needed to obtain reliable estimates from the MCMC sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf86837",
   "metadata": {},
   "source": [
    "We will use `az.summary()` to get for an overview of the posterior distributions. \n",
    "\n",
    "For each parameter, the function shows:\n",
    "\n",
    "* `mean`: the average of the posterior samples\n",
    "\n",
    "* `sd`: standard deviation, showing uncertainty\n",
    "\n",
    "* `hdi_3%` / `hdi_97%`: lower and upper bounds of the 94% highest density interval (by default), representing the range where most values fall\n",
    "\n",
    "* `r_hat`: a convergence diagnostic (should be close to 1.0, or not higher than 1.01)\n",
    "\n",
    "* `ess_bulk` / `ess_tail`: effective sample sizes, it measures how much independent information the samples contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c195fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_no_hierarchy,\n",
    "           var_names=['w_s', \n",
    "                      'w_c', \n",
    "                      'intercept_mu', \n",
    "                      'intercept_offset', \n",
    "                      'mu_extreme', \n",
    "                      'sigma_base', \n",
    "                      'sigma_extreme', \n",
    "                      'p_is_extreme'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccefce",
   "metadata": {},
   "source": [
    "This looks good so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ca0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_no_hierarchy,\n",
    "           var_names=['is_extreme'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a4e27",
   "metadata": {},
   "source": [
    "However, here we start to see issues. Although our model correctly estimates that `is_extreme` is close to 0 for most observations, diagnostics like effective sample size or R-hat show problems. Because we sample `is_extreme` for each observation, we introduce a large number of discrete latent variables (one per data point). \n",
    "\n",
    "Issues happens because:\n",
    "\n",
    "* Gradient-based samplers like NUTS struggle with discrete variables,\n",
    "\n",
    "* Sampling thousands of binary latent variables individually can lead to inefficient exploration and stuck chains.\n",
    "\n",
    "For now, we accept this limitation and continue analyzing the model since we still get reasonable estimates for is_extreme. Fixing these issues would require more advanced steps like marginalizing out the discrete variables or using samplers designed for discrete parameters (project ideas?).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_no_hierarchy, \n",
    "               var_names=['w_s', \n",
    "                          'w_c', \n",
    "                          'intercept_mu', \n",
    "                          'intercept_offset', \n",
    "                          'mu_extreme', \n",
    "                          'sigma_base', \n",
    "                          'sigma_extreme', \n",
    "                          'p_is_extreme'], \n",
    "               combined=True, r_hat=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58570a04",
   "metadata": {},
   "source": [
    "We have a forest plot showing the posterior distributions and 94% credible intervals of selected parameters, with R-hat values to assess convergence across chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633682",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_no_hierarchy, \n",
    "              var_names=['w_s', \n",
    "                         'w_c', \n",
    "                         'intercept_mu', \n",
    "                         'intercept_offset', \n",
    "                         'mu_extreme', \n",
    "                         'sigma_base', \n",
    "                         'sigma_extreme', \n",
    "                         'p_is_extreme'], \n",
    "              compact=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e7bdb",
   "metadata": {},
   "source": [
    "This figure displays the posterior distributions of parameters (left) and the corresponding autocorrelation plots (right) for parameters. The posterior distributions illustrate the range of parameter values after sampling. The autocorrelation plots on the right assess the efficiency and convergence of the MCMC sampling, where lower autocorrelation indicates more efficient sampling. **Different dashed line styles represent the different chains** used during sampling, helping to verify whether the chains have converged to a similar distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845fe7e",
   "metadata": {},
   "source": [
    "## Model 2: County-hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef4334",
   "metadata": {},
   "source": [
    "We will now fit a Bayesian hierarchical model. As mentioned earlier, Bayesian models can include hierarchies of parameters. To start, we’ll add an intercept for each county to capture differences in their baseline PM2.5 level, because it’s expected that these baselines vary across counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8abb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_county_hierarchy:\n",
    "\n",
    "    # --- Priors for seasonality ---\n",
    "    sine = pm.math.sin(2 * np.pi * (monthly_avg[\"month_fraction\"].values % 1))\n",
    "    cosine = pm.math.cos(2 * np.pi * (monthly_avg[\"month_fraction\"].values % 1))\n",
    "\n",
    "    w_s = pm.Normal(\"w_s\", mu=0, sigma=1)\n",
    "    w_c = pm.Normal(\"w_c\", mu=0, sigma=1)\n",
    "\n",
    "    seasonality = sine * w_s + cosine * w_c\n",
    "\n",
    "    # --- Priors for intercept ---\n",
    "\n",
    "\n",
    "    # --- Modeling extreme events ---\n",
    "    # We create a binary indicator variable 'is_extreme' to capture days with unusually high PM2.5.\n",
    "    # The probability of an extreme event is modeled with a Beta prior to allow it to be low but flexible.\n",
    "    \n",
    "    p_is_extreme = pm.Beta(\"p_is_extreme\", alpha=1, beta=20)\n",
    "    is_extreme = pm.Bernoulli(\"is_extreme\", p_is_extreme, shape=monthly_avg.shape[0])  \n",
    "\n",
    "    # --- Expected value of PM2.5 ---\n",
    "    # The baseline expected log PM2.5 is the sum of intercept and seasonal effect (mu_base).\n",
    "   \n",
    "    mu_base = intercept + seasonality\n",
    "\n",
    "    # For extreme events, we allow a separate, higher mean value (mu_extreme) with a fairly wide prior.\n",
    "    mu_extreme = pm.Normal(\"mu_extreme\", mu=5, sigma=5)\n",
    "\n",
    "    # We switch between baseline and extreme expected values depending on 'is_extreme'.\n",
    "    mu = pm.math.switch(is_extreme, mu_extreme, mu_base)\n",
    "\n",
    "    # --- Priors for standard deviations ---\n",
    "    # We model measurement noise or natural variability with two HalfNormal priors:\n",
    "    # one for the baseline condition (sigma_base), and one for extreme events (sigma_extreme).\n",
    "\n",
    "    sigma_base = pm.HalfNormal(\"sigma_base\", sigma=1)\n",
    "    sigma_extreme = pm.HalfNormal(\"sigma_extreme\", sigma=1)\n",
    "\n",
    "    # We switch the standard deviation based on 'is_extreme' status.\n",
    "\n",
    "    sigma = pm.math.switch(is_extreme, sigma_extreme, sigma_base)\n",
    "\n",
    "    # --- Likelihood ---\n",
    "    Y = pm.LogNormal(\"Y\", \n",
    "                    mu=mu, \n",
    "                    sigma=sigma, \n",
    "                    observed=monthly_avg['Monthly Avg PM2.5'])\n",
    "\n",
    "    trace_county_hierarchy = pm.sample(1000, tune=500, chains=4, cores=4, return_inferencedata=True)\n",
    "\n",
    "    pm.compute_log_likelihood(trace_county_hierarchy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5429c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc13b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the model structure using Graphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d310f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_county_hierarchy,\n",
    "           var_names=['w_s', \n",
    "                      'w_c', \n",
    "                      'intercept_mu', \n",
    "                      'intercept_offset', \n",
    "                      'mu_extreme', \n",
    "                      'sigma_base', \n",
    "                      'sigma_extreme', \n",
    "                      'p_is_extreme'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e798771",
   "metadata": {},
   "source": [
    "We see that some `r_hat` values are above 1.01, which suggests convergence issues. This can sometimes be improved by increasing the number of tuning steps, samples, or chains.\n",
    "\n",
    "However, high `r_hat` might also suggest deeper issues with the model, like poor parameterization or parameters that aren’t well identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eee7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot credible intervals for posterior distributions and r_hat using plot_forest function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_county_hierarchy, \n",
    "              var_names=['w_s', \n",
    "                         'w_c', \n",
    "                         'intercept_mu', \n",
    "                         'intercept_offset', \n",
    "                         'mu_extreme', \n",
    "                         'sigma_base', \n",
    "                         'sigma_extreme', \n",
    "                         'p_is_extreme'], \n",
    "              compact=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7482d",
   "metadata": {},
   "source": [
    "## Model 3: County- and trend-hierarchy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fc25c",
   "metadata": {},
   "source": [
    "Since our model with county-specific intercepts still has some issues, we’ll now expand it by adding a trend component.\n",
    "\n",
    "So far, we haven’t explicitly modeled any long-term trend. But interestingly, PM2.5 levels across the US have been __[declining overall](https://www.epa.gov/air-trends/particulate-matter-pm25-trends)__, while in California, the number and intensity of wildfires has increased (see __[here](https://emlab.ucsb.edu/sites/default/files/documents/wildfire-brief.pdf)__ and __[here](https://climateresilience.ca.gov/overview/impacts.html)__). That means while levels of PM2.5 are dropping on average, extreme spikes from wildfires are becoming more common.\n",
    "\n",
    "To capture this, we’ll now give each county its own trend line over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1890af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_county_trend_hierarchy:\n",
    "\n",
    "    # --- Priors for seasonality ---\n",
    "    sine = pm.math.sin(2 * np.pi * (monthly_avg[\"month_fraction\"].values % 1))\n",
    "    cosine = pm.math.cos(2 * np.pi * (monthly_avg[\"month_fraction\"].values % 1))\n",
    "\n",
    "    w_s = pm.Normal(\"w_s\", mu=0, sigma=1)\n",
    "    w_c = pm.Normal(\"w_c\", mu=0, sigma=1)\n",
    "\n",
    "    seasonality = sine * w_s + cosine * w_c\n",
    "\n",
    "    # --- Priors for intercept ---\n",
    "    intercept_mu = pm.Normal(\"intercept_mu\", mu=0, sigma=5)  # wide prior on mean\n",
    "    \n",
    "    intercept_offset = pm.Normal(\"intercept_offset\", mu=0, sigma=1, shape=n_counties)\n",
    "    intercept_raw = intercept_mu + intercept_offset\n",
    "\n",
    "    # intercept hierachy\n",
    "    intercept = intercept_raw[monthly_avg['county_id']]\n",
    "\n",
    "    # --- Priors for trend ---\n",
    "    \n",
    "    # Global average trend across all counties (centered at 0, wide prior)\n",
    "    mu_trend = \n",
    "    \n",
    "    # County-specific deviations from the global trend (one per county)\n",
    "    trend_offset = \n",
    "    \n",
    "    # Combine global and county-specific components to get per-county trend slopes\n",
    "    weight_trend = mu_trend + trend_offset\n",
    "    \n",
    "    # Match each observation to its county's trend slope and multiply by time\n",
    "    trend = \n",
    "\n",
    "    # --- Modeling extreme events ---\n",
    "    # We create a binary indicator variable 'is_extreme' to capture days with unusually high PM2.5.\n",
    "    # The probability of an extreme event is modeled with a Beta prior to allow it to be low but flexible.\n",
    "    \n",
    "    p_is_extreme = pm.Beta(\"p_is_extreme\", alpha=1, beta=50) # very narrow prior\n",
    "    is_extreme = pm.Bernoulli(\"is_extreme\", p_is_extreme, shape=monthly_avg.shape[0])  \n",
    "\n",
    "    # --- Expected value of PM2.5 ---\n",
    "    # The baseline expected log PM2.5 is the sum of intercept and seasonal effect (mu_base).\n",
    "   \n",
    "    mu_base = intercept + seasonality\n",
    "\n",
    "    # For extreme events, we allow a separate, higher mean value (mu_extreme) with a fairly wide prior.\n",
    "    mu_extreme = pm.Normal(\"mu_extreme\", mu=5, sigma=5)\n",
    "\n",
    "    # We switch between baseline and extreme expected values depending on 'is_extreme'.\n",
    "    mu = pm.math.switch(is_extreme, mu_extreme, mu_base)\n",
    "\n",
    "    # --- Priors for standard deviations ---\n",
    "    # We model measurement noise or natural variability with two HalfNormal priors:\n",
    "    # one for the baseline condition (sigma_base), and one for extreme events (sigma_extreme).\n",
    "\n",
    "    sigma_base = pm.HalfNormal(\"sigma_base\", sigma=1)\n",
    "    sigma_extreme = pm.HalfNormal(\"sigma_extreme\", sigma=1)\n",
    "\n",
    "    # We switch the standard deviation based on 'is_extreme' status.\n",
    "\n",
    "    sigma = pm.math.switch(is_extreme, sigma_extreme, sigma_base)\n",
    "\n",
    "    # --- Likelihood ---\n",
    "    Y = pm.LogNormal(\"Y\", \n",
    "                    mu=mu, \n",
    "                    sigma=sigma, \n",
    "                    observed=monthly_avg['Monthly Avg PM2.5'])\n",
    "\n",
    "    trace_county_trend_hierarchy = pm.sample(1000, tune=500, chains=4, cores=4, return_inferencedata=True)\n",
    "\n",
    "    pm.compute_log_likelihood(trace_county_trend_hierarchy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_county_trend_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(model_county_trend_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_county_trend_hierarchy, \n",
    "           var_names=['w_s', \n",
    "                      'w_c', \n",
    "                      'intercept_mu', \n",
    "                      'intercept_offset', \n",
    "                      'mu_extreme', \n",
    "                      'sigma_base', \n",
    "                      'sigma_extreme', \n",
    "                      'p_is_extreme'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace_county_trend_hierarchy, \n",
    "           var_names=['mu_trend',\n",
    "                      'trend_offset']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dcb0b",
   "metadata": {},
   "source": [
    "This looks better than the model with only county intercepts. `r_hat` is still above 1.01 for some parameters, but that might go away if we increase the number of tuning steps or samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259dcddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_county_trend_hierarchy, \n",
    "               var_names=['w_s', \n",
    "                          'w_c', \n",
    "                          'intercept_mu', \n",
    "                          'intercept_offset', \n",
    "                          'mu_extreme', \n",
    "                          'sigma_base', \n",
    "                          'sigma_extreme', \n",
    "                          'p_is_extreme', \n",
    "                          'mu_trend',\n",
    "                          'trend_offset'], \n",
    "               combined=True, \n",
    "               r_hat=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45632610",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_county_trend_hierarchy, \n",
    "              var_names=['w_s', \n",
    "                          'w_c', \n",
    "                          'intercept_mu', \n",
    "                          'intercept_offset', \n",
    "                          'mu_extreme', \n",
    "                          'sigma_base', \n",
    "                          'sigma_extreme', \n",
    "                          'p_is_extreme', \n",
    "                          'mu_trend',\n",
    "                          'trend_offset'], \n",
    "              compact=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85c678",
   "metadata": {},
   "source": [
    "## Generating predictions based on posterior distribution of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcdd0f",
   "metadata": {},
   "source": [
    "We now have 3 models of different complexity. So far we’ve looked at convergence diagnostics, but we haven’t yet compared their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3817099",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"model_no_hierarchy\": {\"model\": model_no_hierarchy, \"trace\": trace_no_hierarchy},\n",
    "    \"model_county_hierarchy\": {\"model\": model_county_hierarchy, \"trace\": trace_county_hierarchy},\n",
    "    \"model_county_trend_hierarchy\": {\"model\": model_county_trend_hierarchy, \"trace\": trace_county_trend_hierarchy},\n",
    "}\n",
    "\n",
    "for label, items in models.items():\n",
    "    ppc = pm.sample_posterior_predictive(items[\"trace\"], model=items[\"model\"], var_names=[\"Y\"])\n",
    "    items[\"ppc\"] = ppc.posterior_predictive[\"Y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharey=True)\n",
    "\n",
    "for ax, (label, items) in zip(axes, models.items()):\n",
    "    samples = items[\"ppc\"].flatten()\n",
    "    sns.kdeplot(samples, ax=ax, label=\"Posterior Predictive\", fill=True, alpha=0.6)\n",
    "    sns.kdeplot(monthly_avg[\"Monthly Avg PM2.5\"], ax=ax, label=\"Observed Data\", color=\"black\", linestyle=\"--\")\n",
    "    \n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"Monthly Avg PM2.5\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "\n",
    "axes[2].set_xlabel(\"Monthly Avg PM2.5\") \n",
    "plt.suptitle(\"Posterior Predictive Check (KDE) per Model\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc5ca0",
   "metadata": {},
   "source": [
    "This figure visually compares the distribution of model-predicted values (orange shade) with the actual observed data (dashed black line). The comparison is based on predictions derived from the model's posterior distribution, checked against the observed data to assess consistency. The misalignment between the two distributions at their peaks and tails suggests that the model may have difficulty accurately predicting certain data points, i.e. outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096806d6",
   "metadata": {},
   "source": [
    "### Compute predictions\n",
    "\n",
    "We will compute the mean of the posterior predictive samples to obtain predictions and calculate percentiles from these samples to represent prediction uncertainty. Although the Bayesian approach also allows for determining credible intervals for model parameters to assess their uncertainty, I will focus on using percentiles of the predictions for easier data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7637a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, items in models.items():\n",
    "    ppc_samples = items[\"ppc\"]  # posterior predictive samples: shape (chains, draws, obs)\n",
    "    trace = items[\"trace\"]\n",
    "    \n",
    "    # Average over chains and draws to get predicted means for each observation\n",
    "    predicted_means = ppc_samples.mean(axis=0).mean(axis=0)\n",
    "    items[\"predicted_means\"] = predicted_means\n",
    "    \n",
    "    # Mean of is_extreme across chains and draws for each observation\n",
    "    is_extreme_mean = trace.posterior[\"is_extreme\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "    items[\"is_extreme_mean\"] = is_extreme_mean\n",
    "    \n",
    "    # Calculate 2.5% and 97.5% quantiles for credible intervals\n",
    "    lower_bound = np.percentile(ppc_samples.mean(axis=0), 2.5, axis=0)\n",
    "    upper_bound = np.percentile(ppc_samples.mean(axis=0), 97.5, axis=0)\n",
    "    items[\"lower_bound\"] = lower_bound\n",
    "    items[\"upper_bound\"] = upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ce13f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['model_no_hierarchy', 'model_county_hierarchy', 'model_county_trend_hierarchy']\n",
    "columns = ['predicted_means', 'lower_bound', 'upper_bound', 'is_extreme_mean']\n",
    "\n",
    "for model in model_names:\n",
    "    for col in columns:\n",
    "        monthly_avg[f'{col}_{model.replace(\"model_\", \"\")}'] = models[model][col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3bd9a",
   "metadata": {},
   "source": [
    "### Calculate r2 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7e74310",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_results = {}\n",
    "\n",
    "# Get unique counties and their names once\n",
    "county_map = monthly_avg.drop_duplicates(subset=['county_id'])[['county_id', 'County']].set_index('county_id')['County']\n",
    "\n",
    "county_ids = monthly_avg['county_id'].unique()\n",
    "\n",
    "for model_label, items in models.items():\n",
    "    r2_scores = []\n",
    "    preds = items['predicted_means']  # predicted means array, length = number of observations\n",
    "    \n",
    "    for county_id in county_ids:\n",
    "        mask = (monthly_avg['county_id'] == county_id)\n",
    "        y_obs = monthly_avg.loc[mask, 'Monthly Avg PM2.5'].values\n",
    "        y_pred = preds[mask.values]  # mask.values to align boolean index with numpy array\n",
    "        \n",
    "        r2 = r2_score(y_obs, y_pred)\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    r2_results[model_label] = r2_scores\n",
    "    \n",
    "    county_to_r2 = dict(zip(county_ids, r2_scores))\n",
    "    \n",
    "    monthly_avg[f'r2_{model_label}'] = monthly_avg['county_id'].map(county_to_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d5c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_avg[monthly_avg[\"County\"]==\"Los Angeles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0bf4f",
   "metadata": {},
   "source": [
    "### Observed vs Predicted plot for each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_counties = ['Los Angeles', 'Fresno', 'San Francisco', 'San Diego']\n",
    "filtered_data = monthly_avg[monthly_avg['County'].isin(plot_counties)]\n",
    "\n",
    "model_to_column = {\n",
    "    \"No Hierarchy\": \"predicted_means_no_hierarchy\",\n",
    "    \"County Hierarchy\": \"predicted_means_county_hierarchy\",\n",
    "    \"County + Trend Hierarchy\": \"predicted_means_county_trend_hierarchy\",\n",
    "}\n",
    "\n",
    "model_to_r2_col = {\n",
    "    \"No Hierarchy\": \"r2_model_no_hierarchy\",\n",
    "    \"County Hierarchy\": \"r2_model_county_hierarchy\",\n",
    "    \"County + Trend Hierarchy\": \"r2_model_county_trend_hierarchy\",\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"No Hierarchy\": \"blue\",\n",
    "    \"County Hierarchy\": \"green\",\n",
    "    \"County + Trend Hierarchy\": \"orange\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, county in zip(axes, plot_counties):\n",
    "    county_data = filtered_data[filtered_data[\"County\"] == county]\n",
    "\n",
    "    min_val = min(county_data['Monthly Avg PM2.5'].min(), *(county_data[col].min() for col in model_to_column.values()))\n",
    "    max_val = max(county_data['Monthly Avg PM2.5'].max(), *(county_data[col].max() for col in model_to_column.values()))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
    "\n",
    "    for model_label, pred_col in model_to_column.items():\n",
    "        r2_val = county_data[model_to_r2_col[model_label]].iloc[0]\n",
    "        ax.scatter(\n",
    "            county_data[\"Monthly Avg PM2.5\"],\n",
    "            county_data[pred_col],\n",
    "            label=f\"{model_label} (R² = {r2_val:.2f})\",\n",
    "            color=colors[model_label],\n",
    "            alpha=0.6,\n",
    "            s=35,\n",
    "        )\n",
    "\n",
    "    ax.set_title(county, fontsize=16)\n",
    "    ax.set_xlabel(\"Observed PM2.5\", fontsize=14)\n",
    "    ax.set_ylabel(\"Predicted PM2.5\", fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861991c9",
   "metadata": {},
   "source": [
    "This plot compares the observed data (x-axis) with the predicted values (y-axis) generated by 3 model for 4 different cities. The diagonal red dashed line represents the line of perfect prediction. Points close to this line indicate better predictions, while scattered points indicate more significant discrepancies between the model’s predictions and the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce175f6",
   "metadata": {},
   "source": [
    "### Predictions for each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e025735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for plotting\n",
    "\n",
    "def plot_model_predictions(data, model_col, lower_col, upper_col, is_extreme_col, plot_counties, title_suffix):\n",
    "    filtered_data = data[data['County'].isin(plot_counties)].copy()\n",
    "    filtered_data = filtered_data.rename(columns={\n",
    "        model_col: 'predicted_means',\n",
    "        lower_col: 'lower_bound',\n",
    "        upper_col: 'upper_bound',\n",
    "        is_extreme_col: 'is_extreme',\n",
    "    })\n",
    "\n",
    "    g = sns.FacetGrid(filtered_data, col='County', col_wrap=2, height=5, aspect=1.5, sharex=False, sharey=False)\n",
    "    \n",
    "    # Disable automatic legend in these plots:\n",
    "    g.map(sns.scatterplot, 'year_month', 'Monthly Avg PM2.5', label='observations', color='black', alpha=0.7, legend=False)\n",
    "    g.map(sns.lineplot, 'year_month', 'predicted_means', label='predictions', color='cornflowerblue', alpha=0.7, legend=False)\n",
    "\n",
    "    fill_patch = patches.Patch(color='lightblue', alpha=0.5, label='95% Percentile')\n",
    "\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    for ax, (region, subset) in zip(g.axes.flat, filtered_data.groupby('County')):\n",
    "        ax.fill_between(subset['year_month'], \n",
    "                        subset['lower_bound'],\n",
    "                        subset['upper_bound'], \n",
    "                        color='lightblue', alpha=0.5)\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.set_ylabel('P(is extreme)', fontsize=12)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        \n",
    "        # Disable automatic legend here too:\n",
    "        sns.lineplot(data=subset, x='year_month', y='is_extreme', ax=ax2, color='red', alpha=0.7, label='P(is extreme)', legend=False)\n",
    "        \n",
    "        if ax == g.axes.flat[0]:\n",
    "            h1, l1 = ax.get_legend_handles_labels()\n",
    "            h2, l2 = ax2.get_legend_handles_labels()\n",
    "            legend_handles.extend(h1)\n",
    "            legend_labels.extend(l1)\n",
    "            legend_handles.extend(h2)\n",
    "            legend_labels.extend(l2)\n",
    "\n",
    "    legend_handles.append(fill_patch)\n",
    "    legend_labels.append('95% Percentile')\n",
    "\n",
    "    for ax, title in zip(g.axes.flat, g.col_names):\n",
    "        ax.set_title(title, fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(g.axes.flat):\n",
    "        x_label = 'year_month' if i in [2, 3] else ''\n",
    "        y_label = 'Monthly Avg PM2.5' if i in [0, 2] else ''\n",
    "        ax.set_xlabel(x_label, fontsize=16 if x_label else None)\n",
    "        ax.set_ylabel(y_label, fontsize=16 if y_label else None)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12, length=6, width=1)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    first_ax = g.axes.flat[0]\n",
    "    first_ax.legend(legend_handles, legend_labels, fontsize=14, loc='upper right')\n",
    "\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    fig = g.figure\n",
    "    fig.suptitle(f\"Model Predictions: {title_suffix}\", fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_predictions(\n",
    "    data=monthly_avg,\n",
    "    model_col='predicted_means_no_hierarchy',\n",
    "    lower_col='lower_bound_no_hierarchy',\n",
    "    upper_col='upper_bound_no_hierarchy',\n",
    "    is_extreme_col='is_extreme_mean_no_hierarchy',\n",
    "    plot_counties=plot_counties,\n",
    "    title_suffix='No Hierarchy'\n",
    ")\n",
    "\n",
    "plot_model_predictions(\n",
    "    data=monthly_avg,\n",
    "    model_col='predicted_means_county_hierarchy',\n",
    "    lower_col='lower_bound_county_hierarchy',\n",
    "    upper_col='upper_bound_county_hierarchy',\n",
    "    is_extreme_col='is_extreme_mean_county_hierarchy',\n",
    "    plot_counties=plot_counties,\n",
    "    title_suffix='County Hierarchy'\n",
    ")\n",
    "\n",
    "plot_model_predictions(\n",
    "    data=monthly_avg,\n",
    "    model_col='predicted_means_county_trend_hierarchy',\n",
    "    lower_col='lower_bound_county_trend_hierarchy',\n",
    "    upper_col='upper_bound_county_trend_hierarchy',\n",
    "    is_extreme_col='is_extreme_mean_county_trend_hierarchy',\n",
    "    plot_counties=plot_counties,\n",
    "    title_suffix='County + Trend Hierarchy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7bb73",
   "metadata": {},
   "source": [
    "Observed data (black dots) and predictions (blue lines) are shown for 4 counties. The shaded areas represent the 5%–95% prediction intervals (the uncertainty rang). Both hierarchical models fit the data somewhat better than the model without hierarchy. The `is_extreme` variable highlights unusual observations, though there is still room for improvement (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7852d46",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5abeee",
   "metadata": {},
   "source": [
    "Let’s compare our three models using `az.compare()`. This shows how well each one generalizes by estimating out-of-sample predictive performance. \n",
    "\n",
    "We should pay attention to these metrics:\n",
    "\n",
    "* **loo (Leave-One-Out cross-validation)**: Lower is better because it means the model predicts new data more accurately.\n",
    "\n",
    "* **p_loo (Effective number of parameters)**: This tells us about model complexity. Higher values mean the model is more complex.\n",
    "\n",
    "* **weight**: This gives the relative chance that a model is the best among the ones compared, based on stacking or averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ed949",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = az.compare({\"model_no_hierarchy\": trace_no_hierarchy, \n",
    "                         \"model_county_hierarchy\": trace_county_hierarchy, \n",
    "                         \"model_county_trend_hierarchy\": trace_county_trend_hierarchy}, \n",
    "                         ic='loo', \n",
    "                         method = 'stacking')\n",
    "display(compare_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_compare(compare_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a63635",
   "metadata": {},
   "source": [
    "We can see that both hierarchical models clearly outperform the one without hierarchy. There’s a slight preference for the model that includes both county and trend hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9addee9",
   "metadata": {},
   "source": [
    "# Conclusion and further steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cb751",
   "metadata": {},
   "source": [
    "This tutorial was an example of building Bayesian hierarchical models in PyMC, model inspection and model comparison. The model picks up on differences between counties and detects extreme events, it could still be improved.\n",
    "\n",
    "Some ways to improve the model:\n",
    "\n",
    "1. Add weather data such as average, minimum, and maximum temperature and humidity to better explain PM2.5 variation.\n",
    "\n",
    "2. Include wildfire data, since PM2.5 spikes are often wildfire-related, though other pollution sources exist.\n",
    "\n",
    "3. Investigate clusters in the data and consider adding a cluster-level hierarchy. For example, coastal vs. inland California differ in climate and pollution patterns, though urbanization might affect this relationship.\n",
    "\n",
    "4. Experiment with the trend structure. Currently, each county has its own trend,  but a global trend might be enough.\n",
    "\n",
    "5. Revisit priors and their parameters. The current choices worked well for the dataset but should be reevaluated as you add more data.\n",
    "\n",
    "6. Examine issues with the `is_extreme` variable and its detection capabilities.\n",
    "\n",
    "7. Consider using daily data instead of monthly averages. This would require modeling daily seasonality and accounting for weekday/holiday effects, but would increase model complexity and runtime.\n",
    "\n",
    "6. Add your own ideas :)\n",
    "\n",
    "After developing a model using data from 2019 to 2024, it would be good to test its predictive performance on 2025 data (unseen during training). Keep in mind that data availability for 2025 may vary across counties.\n",
    "\n",
    "Data Sources: \n",
    "* https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/county/time-series\n",
    "* https://catalog.data.gov/dataset?tags=wildfire\n",
    "* https://projects.capradio.org/california-fire-history/\n",
    "* https://scholars.georgiasouthern.edu/en/datasets/california-weather-and-fire-prediction-dataset-19842025-with-engi\n",
    "* https://data.ca.gov/dataset/cal-fire\n",
    "* https://www.kaggle.com/datasets/vivekattri/california-wildfire-damage-2014-feb2025\n",
    "* https://zenodo.org/records/14712845\n",
    "\n",
    "Keep in mind that some of these datasets are on the state or national level, some are on the county-level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c28c4",
   "metadata": {},
   "source": [
    "Links to some interesting papers about Bayesian modeling and PM2.5 I found (no need to implement these models, they're just a good starting point for further reading and inspiration):\n",
    "\n",
    "* https://www.nature.com/articles/s41370-022-00471-4\n",
    "* https://pmc.ncbi.nlm.nih.gov/articles/PMC8102649/\n",
    "* https://onlinelibrary.wiley.com/doi/10.1155/2020/7135142\n",
    "* https://ehp.niehs.nih.gov/doi/10.1289/ehp.6980\n",
    "* https://doi.org/10.1371/journal.pone.0272774\n",
    "* https://link.springer.com/article/10.1007/s11869-025-01762-z\n",
    "* https://www.sciencedirect.com/science/article/pii/S0160412018303891\n",
    "* https://ajehe.umsha.ac.ir/Article/58031\n",
    "* https://www.mdpi.com/2073-4433/15/5/594\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (predict-demand)",
   "language": "python",
   "name": "predict-demand"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
